{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19d9300a",
      "metadata": {
        "id": "19d9300a"
      },
      "source": [
        "\n",
        "# Embeddings Comparison: Word2Vec, FastText, GloVe, spaCy, SBERT, Raw BERT\n",
        "Run cells top-to-bottom. Edit the **Data** section to plug in your own corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0f2f1c",
      "metadata": {
        "id": "3d0f2f1c"
      },
      "source": [
        "## 1) Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk sentence-transformers spacy scikit-learn matplotlib umap-learn transformers torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xqkeaONsUDCz"
      },
      "id": "xqkeaONsUDCz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6d22bc",
      "metadata": {
        "id": "3c6d22bc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "# Visualization & eval\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import umap\n",
        "\n",
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# Sentence Transformers (SBERT)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# spaCy (quick sentence/word vecs)\n",
        "import spacy\n",
        "\n",
        "# Transformers (raw BERT)\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "print(\"✅ Imports loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a20c2ba",
      "metadata": {
        "id": "3a20c2ba"
      },
      "source": [
        "## 2) Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5898f5",
      "metadata": {
        "id": "0e5898f5"
      },
      "outputs": [],
      "source": [
        "\n",
        "RAW_TEXT = \"\"\"\n",
        "King and queen rule a kingdom. A man and a woman walk to Rome in Italy.\n",
        "Paris is the capital of France. Apple and banana are fruits.\n",
        "The queen and the woman visited Paris. The king and the man visited Rome.\n",
        "A royal family governs the kingdom, and their palace is in the capital.\n",
        "Italy and France are European countries. Rome and Paris are famous cities.\n",
        "Apples and bananas are often found in markets across the cities.\n",
        "\"\"\"\n",
        "\n",
        "SENTENCES = [\n",
        "    \"Paris is the capital of France.\",\n",
        "    \"Rome is the capital of Italy.\",\n",
        "    \"Apples and bananas are common fruits.\",\n",
        "    \"The king and the queen live in a palace.\",\n",
        "    \"A woman walked to Rome.\",\n",
        "    \"Bananas are sold in city markets.\",\n",
        "    \"France and Italy are European countries.\",\n",
        "    \"The royal family rules the kingdom.\",\n",
        "]\n",
        "\n",
        "CONTENT_WORDS = [\n",
        "    \"king\",\"queen\",\"man\",\"woman\",\n",
        "    \"paris\",\"france\",\"rome\",\"italy\",\n",
        "    \"kingdom\",\"palace\",\"capital\",\"country\",\"city\",\n",
        "    \"apple\",\"banana\",\"fruit\",\"family\",\"royal\"\n",
        "]\n",
        "\n",
        "print(\"✅ Data loaded (edit RAW_TEXT/SENTENCES for your corpus)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0450e84e",
      "metadata": {
        "id": "0450e84e"
      },
      "source": [
        "## 3) Sentence Split + Tokenization (stopwords removed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcac060c",
      "metadata": {
        "id": "bcac060c"
      },
      "outputs": [],
      "source": [
        "\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def sentence_split(text: str):\n",
        "    # Simple, dependency-light split\n",
        "    return [s.strip() for s in re.split(r\"[.\\n!?]+\", text) if s.strip()]\n",
        "\n",
        "def tokenize_line(line: str):\n",
        "    toks = [t.lower() for t in word_tokenize(line) if t.isalpha()]\n",
        "    toks = [t for t in toks if t not in stops]  # remove stopwords\n",
        "    return toks\n",
        "\n",
        "sentences_raw = sentence_split(RAW_TEXT)\n",
        "tokenized = [tokenize_line(s) for s in sentences_raw]\n",
        "\n",
        "print(\"=== Tokenized sentences (first 8) ===\")\n",
        "for i, s in enumerate(tokenized[:8], 1):\n",
        "    print(f\"{i:02d}:\", s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76fbb975",
      "metadata": {
        "id": "76fbb975"
      },
      "source": [
        "## 4) Train Word2Vec & FastText (gensim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f137a9",
      "metadata": {
        "id": "92f137a9"
      },
      "outputs": [],
      "source": [
        "\n",
        "w2v = Word2Vec(\n",
        "    sentences=tokenized,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,       # keep low for demo; raise to >=5 on real corpora\n",
        "    workers=4,\n",
        "    sg=1,              # skip-gram\n",
        "    negative=10,\n",
        "    sample=1e-3,\n",
        "    epochs=200,\n",
        "    seed=7\n",
        ")\n",
        "\n",
        "ft = FastText(\n",
        "    sentences=tokenized,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    sg=1,\n",
        "    negative=10,\n",
        "    sample=1e-3,\n",
        "    epochs=200,\n",
        "    seed=7\n",
        ")\n",
        "\n",
        "w2v_kv = w2v.wv\n",
        "ft_kv  = ft.wv\n",
        "\n",
        "print(\"✅ Trained Word2Vec & FastText\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dbe3777",
      "metadata": {
        "id": "5dbe3777"
      },
      "source": [
        "## 5) Load Pretrained: GloVe, spaCy, SBERT, Raw BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b888f3ce",
      "metadata": {
        "id": "b888f3ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Loading pretrained models (GloVe, spaCy, SBERT, BERT)...\")\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "# GloVe word vectors\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# spaCy medium model; exclude lemmatizer to avoid W108 warning\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# SBERT sentence embeddings\n",
        "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Raw BERT for mean-pooled sentence embeddings\n",
        "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model.eval()\n",
        "\n",
        "print(\"✅ Pretrained models ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9fa3e7",
      "metadata": {
        "id": "dd9fa3e7"
      },
      "source": [
        "## 6) Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e12bc10",
      "metadata": {
        "id": "6e12bc10"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cos(a, b):\n",
        "    a = np.asarray(a); b = np.asarray(b)\n",
        "    return float(np.dot(a, b) / ((np.linalg.norm(a) + 1e-9)*(np.linalg.norm(b) + 1e-9)))\n",
        "\n",
        "def vec_gensim_word(model, word):\n",
        "    return model[word] if word in model.key_to_index else None\n",
        "\n",
        "def vec_glove_word(word):\n",
        "    return glove[word] if word in glove.key_to_index else None\n",
        "\n",
        "def embed_spacy(texts):\n",
        "    # doc.vector: averaged pretrained word vectors\n",
        "    docs = list(nlp.pipe(texts, disable=[\"tagger\",\"parser\",\"ner\"]))\n",
        "    X = np.vstack([d.vector for d in docs])\n",
        "    X = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
        "    return X\n",
        "\n",
        "def sbert_embed(texts):\n",
        "    return sbert.encode(texts, normalize_embeddings=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def bert_sentence_embed(texts):\n",
        "    enc = bert_tok(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    out = bert_model(**enc).last_hidden_state  # (B, T, H)\n",
        "    mask = enc[\"attention_mask\"].unsqueeze(-1)\n",
        "    summed = (out * mask).sum(dim=1)\n",
        "    counts = mask.sum(dim=1).clamp(min=1)\n",
        "    mean = (summed / counts).cpu().numpy()\n",
        "    mean = mean / (np.linalg.norm(mean, axis=1, keepdims=True) + 1e-9)\n",
        "    return mean\n",
        "\n",
        "print(\"✅ Utilities ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc4df47d",
      "metadata": {
        "id": "cc4df47d"
      },
      "source": [
        "## 7) Sample Embeddings (Word2Vec / FastText / GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "976032e8",
      "metadata": {
        "id": "976032e8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_vector(label, vec, head=8):\n",
        "    if vec is None:\n",
        "        print(f\"(missing) '{label}' has no vector\")\n",
        "        return\n",
        "    print(f\"\\nVector for '{label}' (first {head} dims of {len(vec)}):\")\n",
        "    print(np.array2string(vec[:head], precision=4, suppress_small=True))\n",
        "\n",
        "for w in [\"king\",\"queen\",\"paris\",\"france\",\"rome\",\"italy\",\"apple\",\"banana\",\"palace\",\"capital\"]:\n",
        "    show_vector(f\"W2V:{w}\", vec_gensim_word(w2v_kv, w))\n",
        "    show_vector(f\"FT :{w}\", vec_gensim_word(ft_kv, w))\n",
        "    show_vector(f\"GloVe:{w}\", vec_glove_word(w))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "826530bc",
      "metadata": {
        "id": "826530bc"
      },
      "source": [
        "## 7b) Sentence-level vectors + cosine similarities + heatmaps (spaCy / SBERT / BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0d21b9",
      "metadata": {
        "id": "fc0d21b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "sent_samples = [\n",
        "    \"The king and the queen live in a palace.\",\n",
        "    \"Paris is the capital of France.\",\n",
        "    \"Apples and bananas are common fruits.\",\n",
        "    \"A woman walked to Rome.\"\n",
        "]\n",
        "\n",
        "X_spacy_samp = embed_spacy(sent_samples)\n",
        "X_sbert_samp = sbert_embed(sent_samples)\n",
        "X_bert_samp  = bert_sentence_embed(sent_samples)\n",
        "\n",
        "def show_sent_vectors(name, X, texts, head=8):\n",
        "    print(f\"\\n=== {name} sentence vectors ===\")\n",
        "    for s, v in zip(texts, X):\n",
        "        print(f\"\\n{s}\")\n",
        "        print(f\"first {head} of {len(v)} dims:\")\n",
        "        print(np.array2string(v[:head], precision=4, suppress_small=True))\n",
        "\n",
        "show_sent_vectors(\"spaCy\", X_spacy_samp, sent_samples)\n",
        "show_sent_vectors(\"SBERT\", X_sbert_samp, sent_samples)\n",
        "show_sent_vectors(\"BERT \", X_bert_samp,  sent_samples)\n",
        "\n",
        "def cosine_matrix(X):\n",
        "    return (X @ X.T)  # X is already normalized\n",
        "\n",
        "def print_cosine_table(name, sims, texts):\n",
        "    print(f\"\\n=== Cosine similarities ({name}) ===\")\n",
        "    n = sims.shape[0]\n",
        "    for i in range(n):\n",
        "        row = \" | \".join(f\"{sims[i,j]:.2f}\" for j in range(n))\n",
        "        print(f\"{i:02d} {row}  <- {texts[i]}\")\n",
        "\n",
        "S_spacy = cosine_matrix(X_spacy_samp)\n",
        "S_sbert = cosine_matrix(X_sbert_samp)\n",
        "S_bert  = cosine_matrix(X_bert_samp)\n",
        "\n",
        "print_cosine_table(\"spaCy\", S_spacy, sent_samples)\n",
        "print_cosine_table(\"SBERT\", S_sbert, sent_samples)\n",
        "print_cosine_table(\"BERT \", S_bert,  sent_samples)\n",
        "\n",
        "def plot_heatmap(sim, title, labels):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.imshow(sim, aspect='auto')\n",
        "    plt.title(title)\n",
        "    plt.xticks(range(len(labels)), range(len(labels)))\n",
        "    plt.yticks(range(len(labels)), range(len(labels)))\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_heatmap(S_spacy, \"Cosine (spaCy) – sentence samples\", sent_samples)\n",
        "plot_heatmap(S_sbert, \"Cosine (SBERT) – sentence samples\", sent_samples)\n",
        "plot_heatmap(S_bert,  \"Cosine (BERT) – sentence samples\", sent_samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd8ffc1",
      "metadata": {
        "id": "2bd8ffc1"
      },
      "source": [
        "## 8) Cosine Similarities (word-level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec72a3b7",
      "metadata": {
        "id": "ec72a3b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "pairs = [\n",
        "    (\"king\",\"queen\"),\n",
        "    (\"king\",\"man\"),\n",
        "    (\"queen\",\"woman\"),\n",
        "    (\"paris\",\"france\"),\n",
        "    (\"rome\",\"italy\"),\n",
        "    (\"apple\",\"banana\"),\n",
        "    (\"king\",\"apple\"),\n",
        "]\n",
        "\n",
        "def cosine_table(name, vec_fn):\n",
        "    print(f\"\\n=== Cosine similarities ({name}) ===\")\n",
        "    for a, b in pairs:\n",
        "        va, vb = vec_fn(a), vec_fn(b)\n",
        "        if va is not None and vb is not None:\n",
        "            print(f\"{a:>6} ~ {b:<6}: {cos(va, vb):.3f}\")\n",
        "\n",
        "cosine_table(\"Word2Vec\", lambda w: vec_gensim_word(w2v_kv, w))\n",
        "cosine_table(\"FastText\", lambda w: vec_gensim_word(ft_kv, w))\n",
        "cosine_table(\"GloVe\",   vec_glove_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704549c6",
      "metadata": {
        "id": "704549c6"
      },
      "source": [
        "## 9) Analogy: king - man + woman (no temp vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de441f0",
      "metadata": {
        "id": "8de441f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def analogy(model, a, b, c, topn=5):\n",
        "    if not all(w in model.key_to_index for w in [a,b,c]): return []\n",
        "    return model.most_similar(positive=[a, c], negative=[b], topn=topn)\n",
        "\n",
        "print(\"\\n=== Analogy: king - man + woman ≈ ? ===\")\n",
        "print(\"Word2Vec ->\"); pprint(analogy(w2v_kv, \"king\",\"man\",\"woman\"))\n",
        "print(\"FastText ->\"); pprint(analogy(ft_kv,  \"king\",\"man\",\"woman\"))\n",
        "print(\"GloVe   ->\");  pprint(glove.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"], topn=5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06654b11",
      "metadata": {
        "id": "06654b11"
      },
      "source": [
        "## 10) Sentence Semantic Search & Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d5c8e9",
      "metadata": {
        "id": "67d5c8e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_sbert_all = sbert_embed(SENTENCES)\n",
        "X_bert_all  = bert_sentence_embed(SENTENCES)\n",
        "X_spacy_all = embed_spacy(SENTENCES)\n",
        "\n",
        "def semantic_search(query, X, encoder, topk=3):\n",
        "    q = encoder([query])\n",
        "    S = cosine_similarity(q, X)[0]\n",
        "    order = np.argsort(S)[::-1][:topk]\n",
        "    return [(SENTENCES[i], float(S[i])) for i in order]\n",
        "\n",
        "queries = [\n",
        "    \"capital of a country\",\n",
        "    \"royal family and palace\",\n",
        "    \"common fruits in markets\",\n",
        "]\n",
        "\n",
        "print(\"\\n=== Sentence semantic search ===\")\n",
        "for name, X, enc in [\n",
        "    (\"SBERT\", X_sbert_all, sbert_embed),\n",
        "    (\"BERT \", X_bert_all,  bert_sentence_embed),\n",
        "    (\"spaCy\", X_spacy_all, embed_spacy),\n",
        "]:\n",
        "    print(f\"\\n-- {name} --\")\n",
        "    for q in queries:\n",
        "        print(q, \"->\")\n",
        "        pprint(semantic_search(q, X, enc))\n",
        "\n",
        "for name, X in [(\"SBERT\", X_sbert_all), (\"BERT\", X_bert_all), (\"spaCy\", X_spacy_all)]:\n",
        "    k = 3\n",
        "    km = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
        "    sil = silhouette_score(X, km.labels_)\n",
        "    print(f\"{name} clustering silhouette (k={k}): {sil:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43f340d",
      "metadata": {
        "id": "a43f340d"
      },
      "source": [
        "## 11) 3D PCA Word Plot with Cosine Edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e965885a",
      "metadata": {
        "id": "e965885a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def word_plot_3d_with_edges(model_kv, title, words, edge_threshold=0.55):\n",
        "    words = [w for w in words if w in model_kv.key_to_index]\n",
        "    if len(words) < 3:\n",
        "        print(f\"Not enough words for plot: {title}\")\n",
        "        return\n",
        "    X = np.stack([model_kv[w] for w in words])\n",
        "\n",
        "    pca = PCA(n_components=3, random_state=0)\n",
        "    X3 = pca.fit_transform(X)\n",
        "\n",
        "    Vn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
        "    S = (Vn @ Vn.T)  # cosine sim matrix\n",
        "\n",
        "    fig = plt.figure(figsize=(9,7))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.scatter(X3[:,0], X3[:,1], X3[:,2])\n",
        "    for i, w in enumerate(words):\n",
        "        ax.text(X3[i,0], X3[i,1], X3[i,2], w)\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        for j in range(i+1, len(words)):\n",
        "            if S[i, j] >= edge_threshold:\n",
        "                xs = [X3[i,0], X3[j,0]]\n",
        "                ys = [X3[i,1], X3[j,1]]\n",
        "                zs = [X3[i,2], X3[j,2]]\n",
        "                ax.plot(xs, ys, zs, linewidth=1 + 2*(S[i,j]-edge_threshold),\n",
        "                        alpha=min(0.2 + 0.8*(S[i,j]-edge_threshold), 1.0))\n",
        "\n",
        "                if (words[i], words[j]) in [(\"king\",\"queen\"), (\"paris\",\"france\"),\n",
        "                                            (\"rome\",\"italy\"), (\"apple\",\"banana\")] or                    (words[j], words[i]) in [(\"king\",\"queen\"), (\"paris\",\"france\"),\n",
        "                                            (\"rome\",\"italy\"), (\"apple\",\"banana\")]:\n",
        "                    mx = (xs[0]+xs[1])/2; my = (ys[0]+ys[1])/2; mz = (zs[0]+zs[1])/2\n",
        "                    ax.text(mx, my, mz, f\"{S[i,j]:.2f}\")\n",
        "\n",
        "    ax.set_title(title + \"\\nEdges = cosine similarity ≥ \" + str(edge_threshold))\n",
        "    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.set_zlabel(\"PC3\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Generating 3D plots...\")\n",
        "word_plot_3d_with_edges(w2v_kv, \"Word2Vec Embeddings (PCA → 3D)\", CONTENT_WORDS, edge_threshold=0.55)\n",
        "word_plot_3d_with_edges(ft_kv,  \"FastText Embeddings (PCA → 3D)\", CONTENT_WORDS, edge_threshold=0.55)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e663778",
      "metadata": {
        "id": "6e663778"
      },
      "source": [
        "## 11b) Sentence embeddings: 3D PCA (SBERT vs BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d78d44c",
      "metadata": {
        "id": "0d78d44c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_sentences_3d(X, title, labels):\n",
        "    p3 = PCA(n_components=3, random_state=0).fit_transform(X)\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.scatter(p3[:,0], p3[:,1], p3[:,2])\n",
        "    for i, txt in enumerate(labels):\n",
        "        ax.text(p3[i,0], p3[i,1], p3[i,2], str(i))\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.set_zlabel(\"PC3\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "    for i, s in enumerate(labels):\n",
        "        print(f\"{i:02d}: {s}\")\n",
        "\n",
        "plot_sentences_3d(X_sbert_all, \"SBERT – Sentences (PCA 3D)\", SENTENCES)\n",
        "plot_sentences_3d(X_bert_all,  \"BERT  – Sentences (PCA 3D)\", SENTENCES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef10473a",
      "metadata": {
        "id": "ef10473a"
      },
      "source": [
        "## 12) (Optional) UMAP 2D Visualizations (words & sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5fe60ae",
      "metadata": {
        "id": "b5fe60ae"
      },
      "outputs": [],
      "source": [
        "\n",
        "def word_umap_plot(model_kv, title, words):\n",
        "    words = [w for w in words if w in model_kv.key_to_index]\n",
        "    if len(words) < 3:\n",
        "        print(f\"Not enough words for plot: {title}\")\n",
        "        return\n",
        "    X = np.stack([model_kv[w] for w in words])\n",
        "    X2 = umap.UMAP(n_components=2, random_state=0, n_neighbors=10, min_dist=0.1).fit_transform(X)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(X2[:,0], X2[:,1])\n",
        "    for i,w in enumerate(words):\n",
        "        plt.text(X2[i,0]+0.01, X2[i,1]+0.01, w)\n",
        "    plt.title(title); plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_sentences_umap(X, title, labels):\n",
        "    u = umap.UMAP(n_components=2, random_state=0, n_neighbors=8, min_dist=0.1).fit_transform(X)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(u[:,0], u[:,1])\n",
        "    for i, txt in enumerate(labels):\n",
        "        plt.text(u[i,0]+0.01, u[i,1]+0.01, str(i))\n",
        "    plt.title(title); plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "    for i, s in enumerate(labels):\n",
        "        print(f\"{i:02d}: {s}\")\n",
        "\n",
        "# Word 2D plots (uncomment to run)\n",
        "# word_umap_plot(w2v_kv, \"Word2Vec (UMAP 2D)\", CONTENT_WORDS)\n",
        "# word_umap_plot(ft_kv,  \"FastText (UMAP 2D)\", CONTENT_WORDS)\n",
        "\n",
        "# Sentence 2D plots (SBERT vs BERT)\n",
        "plot_sentences_umap(X_sbert_all, \"SBERT – Sentences (UMAP 2D)\", SENTENCES)\n",
        "plot_sentences_umap(X_bert_all,  \"BERT  – Sentences (UMAP 2D)\", SENTENCES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da20162",
      "metadata": {
        "id": "7da20162"
      },
      "source": [
        "## 13) Word analogy arrows in 3D (PCA) and 2D (UMAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f69b6b",
      "metadata": {
        "id": "a9f69b6b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def analogy_target(model_kv, a, b, c):\n",
        "    if not all(w in model_kv.key_to_index for w in [a,b,c]):\n",
        "        return None, []\n",
        "    pred = model_kv[a] - model_kv[b] + model_kv[c]\n",
        "    V = model_kv.vectors\n",
        "    Vn = V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-9)\n",
        "    p = pred / (np.linalg.norm(pred) + 1e-9)\n",
        "    sims = Vn @ p\n",
        "    idx2w = [None]*len(model_kv.key_to_index)\n",
        "    for w, i in model_kv.key_to_index.items():\n",
        "        idx2w[i] = w\n",
        "    order = np.argsort(sims)[::-1]\n",
        "    best = []\n",
        "    for i in order:\n",
        "        w = idx2w[i]\n",
        "        if w in {a,b,c}:\n",
        "            continue\n",
        "        best.append((w, float(sims[i])))\n",
        "        if len(best) == 5:\n",
        "            break\n",
        "    return pred, best\n",
        "\n",
        "def word_analogy_plot_3d(model_kv, title, words, triplet=(\"king\",\"man\",\"woman\")):\n",
        "    words = [w for w in words if w in model_kv.key_to_index]\n",
        "    if not words:\n",
        "        print(\"No words to plot\");\n",
        "        return\n",
        "    X = np.stack([model_kv[w] for w in words])\n",
        "    p3 = PCA(n_components=3, random_state=0).fit_transform(X)\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.scatter(p3[:,0], p3[:,1], p3[:,2])\n",
        "    for i, w in enumerate(words):\n",
        "        ax.text(p3[i,0], p3[i,1], p3[i,2], w)\n",
        "    a,b,c = triplet\n",
        "    if all(w in model_kv.key_to_index for w in [a,b,c]):\n",
        "        pred_vec, best = analogy_target(model_kv, a,b,c)\n",
        "        if best:\n",
        "            target = best[0][0]\n",
        "            def proj(w): return p3[words.index(w)]\n",
        "            try:\n",
        "                A,B,C,T = proj(a), proj(b), proj(c), proj(target)\n",
        "                ax.plot([A[0],B[0]],[A[1],B[1]],[A[2],B[2]])\n",
        "                ax.plot([B[0],C[0]],[B[1],C[1]],[B[2],C[2]])\n",
        "                ax.plot([C[0],T[0]],[C[1],T[1]],[C[2],T[2]])\n",
        "                ax.text(T[0], T[1], T[2], f\"≈ {target}\")\n",
        "            except ValueError:\n",
        "                pass\n",
        "    ax.set_title(title + \" – Analogy arrows\")\n",
        "    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.set_zlabel(\"PC3\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def word_analogy_plot_2d(model_kv, title, words, triplet=(\"king\",\"man\",\"woman\")):\n",
        "    words = [w for w in words if w in model_kv.key_to_index]\n",
        "    if len(words) < 3:\n",
        "        print(\"Not enough words\"); return\n",
        "    X = np.stack([model_kv[w] for w in words])\n",
        "    u = umap.UMAP(n_components=2, random_state=0, n_neighbors=8, min_dist=0.1).fit_transform(X)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(u[:,0], u[:,1])\n",
        "    for i, w in enumerate(words):\n",
        "        plt.text(u[i,0]+0.01, u[i,1]+0.01, w)\n",
        "    a,b,c = triplet\n",
        "    if all(w in model_kv.key_to_index for w in [a,b,c]) and all(w in words for w in [a,b,c]):\n",
        "        pred_vec, best = analogy_target(model_kv, a,b,c)\n",
        "        if best:\n",
        "            target = best[0][0]\n",
        "            A = u[words.index(a)]; B = u[words.index(b)]\n",
        "            Cp = u[words.index(c)]; T = u[words.index(target)]\n",
        "            plt.plot([A[0],B[0]],[A[1],B[1]])\n",
        "            plt.plot([B[0],Cp[0]],[B[1],Cp[1]])\n",
        "            plt.plot([Cp[0],T[0]],[Cp[1],T[1]])\n",
        "            plt.text(T[0], T[1], f\"≈ {target}\")\n",
        "    plt.title(title + \" – Analogy arrows\")\n",
        "    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Draw for W2V/FT; GloVe via wrapper\n",
        "word_analogy_plot_3d(w2v_kv, \"W2V (PCA 3D)\", CONTENT_WORDS, (\"king\",\"man\",\"woman\"))\n",
        "word_analogy_plot_3d(ft_kv,  \"FastText (PCA 3D)\", CONTENT_WORDS, (\"king\",\"man\",\"woman\"))\n",
        "\n",
        "class SimpleKV:\n",
        "    def __init__(self, words):\n",
        "        self.key_to_index = {w:i for i,w in enumerate(words)}\n",
        "        self.vecs = np.stack([glove[w] for w in words])\n",
        "    def __getitem__(self, w): return self.vecs[self.key_to_index[w]]\n",
        "    @property\n",
        "    def vectors(self): return self.vecs\n",
        "\n",
        "glove_words = [w for w in CONTENT_WORDS if w in glove.key_to_index]\n",
        "if len(glove_words) >= 4:\n",
        "    gkv = SimpleKV(glove_words)\n",
        "    word_analogy_plot_3d(gkv, \"GloVe (PCA 3D)\", glove_words, (\"king\",\"man\",\"woman\"))\n",
        "    word_analogy_plot_2d(w2v_kv, \"W2V (UMAP 2D)\", CONTENT_WORDS, (\"king\",\"man\",\"woman\"))\n",
        "    word_analogy_plot_2d(ft_kv,  \"FastText (UMAP 2D)\", CONTENT_WORDS, (\"king\",\"man\",\"woman\"))\n",
        "    word_analogy_plot_2d(gkv,    \"GloVe (UMAP 2D)\", glove_words, (\"king\",\"man\",\"woman\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}